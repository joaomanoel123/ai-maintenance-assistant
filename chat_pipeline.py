"""
===============================================================================
CHAT PIPELINE - L√≥gica de Chat com Streaming (Hugging Face)
===============================================================================

Implementa:
- Recupera√ß√£o de contexto da mem√≥ria vetorial
- Detec√ß√£o de inten√ß√£o de predi√ß√£o
- Gera√ß√£o com Hugging Face (local ou API)
- Persist√™ncia de experi√™ncias

Autor: Jo√£o Manoel
===============================================================================
"""

import asyncio
import logging
import re
from typing import AsyncGenerator, Dict, Optional, Any
from datetime import datetime

from embeddings import VectorMemory
from model_loader import model_predict
from db import save_experience_record
from llm_huggingface import generate_response_stream
from utils_llm import format_prompt_for_chat

logger = logging.getLogger(__name__)

# ============================================
# DETEC√á√ÉO DE INTEN√á√ÉO
# ============================================

def extract_prediction_intent(message: str) -> Optional[Dict[str, Any]]:
    """
    Detectar se usu√°rio quer fazer uma predi√ß√£o
    
    Returns:
        Dict com tipo e dados se detectado, None caso contr√°rio
    """
    message_lower = message.lower()
    
    # Palavras-chave para RUL
    rul_keywords = ['rul', 'vida √∫til', 'vida util', 'quanto tempo', 'ciclos', 'cmapss']
    
    # Palavras-chave para Falha
    failure_keywords = ['falha', 'failure', 'quebra', 'defeito', 'ai4i', 'probabilidade']
    
    # Verificar RUL
    if any(kw in message_lower for kw in rul_keywords):
        return {"type": "rul", "model": "cmapss"}
    
    # Verificar Falha
    if any(kw in message_lower for kw in failure_keywords):
        return {"type": "failure", "model": "ai4i"}
    
    return None

# ============================================
# CONSTRU√á√ÉO DE PROMPT
# ============================================

def build_llm_prompt(
    user_message: str,
    contexts: list,
    prediction_info: Optional[Dict] = None
) -> str:
    """
    Construir prompt para Hugging Face
    
    Args:
        user_message: Mensagem do usu√°rio
        contexts: Contextos recuperados da mem√≥ria
        prediction_info: Informa√ß√µes de predi√ß√£o (opcional)
    
    Returns:
        Prompt formatado
    """
    # System prompt
    system_prompt = """Voc√™ √© um assistente especializado em AGI (Intelig√™ncia Artificial Geral) e manuten√ß√£o preditiva.
Seu papel √© ajudar usu√°rios a entender conceitos de RUL (Remaining Useful Life), predi√ß√£o de falhas, 
e an√°lise de dados de sensores industriais. Seja claro, t√©cnico quando necess√°rio, mas acess√≠vel."""
    
    # Contexto da mem√≥ria
    context_text = ""
    if contexts:
        context_text = "Informa√ß√µes relevantes:\n"
        for i, ctx in enumerate(contexts[:3], 1):
            context_text += f"{i}. {ctx}\n"
    
    # Informa√ß√£o de predi√ß√£o
    if prediction_info:
        context_text += f"\nPredi√ß√£o realizada:\n{prediction_info}\n"
    
    # Formatar prompt
    prompt = format_prompt_for_chat(
        system_prompt=system_prompt,
        user_message=user_message,
        context=context_text if context_text else None
    )
    
    return prompt

# ============================================
# GERA√á√ÉO DE RESPOSTA COM HUGGING FACE
# ============================================

async def respond_stream_generator(
    user_message: str,
    user_id: str,
    memory: VectorMemory,
    models: Dict
) -> AsyncGenerator[str, None]:
    """
    Gerar resposta usando Hugging Face com streaming
    
    Yields:
        Chunks da resposta
    """
    try:
        logger.info(f"ü§ñ Processando mensagem: {user_message[:50]}...")
        
        # 1. RECUPERAR CONTEXTO DA MEM√ìRIA
        results = memory.query(user_message, n_results=5)
        contexts = results['documents'][0] if results and results['documents'] else []
        
        logger.info(f"üìö Contextos recuperados: {len(contexts)}")
        
        # 2. DETECTAR INTEN√á√ÉO DE PREDI√á√ÉO
        prediction_intent = extract_prediction_intent(user_message)
        prediction_info = None
        
        if prediction_intent and models.get(prediction_intent['model']):
            logger.info(f"üéØ Inten√ß√£o de predi√ß√£o detectada: {prediction_intent['type']}")
            
            # Fazer predi√ß√£o
            try:
                if prediction_intent['model'] == 'cmapss':
                    features = [520.0] * 21
                    pred_result = model_predict('cmapss', features)
                    rul_value = pred_result.get('rul', 'N/A')
                    prediction_info = f"RUL estimado: {rul_value} ciclos"
                
                elif prediction_intent['model'] == 'ai4i':
                    features = [1, 300, 310, 1500, 40, 100]
                    pred_result = model_predict('ai4i', features)
                    prob = pred_result.get('probability', 0)
                    prediction_info = f"Probabilidade de falha: {prob*100:.1f}%"
                
                logger.info(f"‚úÖ Predi√ß√£o: {prediction_info}")
            
            except Exception as e:
                logger.error(f"‚ùå Erro na predi√ß√£o: {e}")
                prediction_info = "Erro ao realizar predi√ß√£o"
        
        # 3. CONSTRUIR PROMPT PARA LLM
        prompt = build_llm_prompt(user_message, contexts, prediction_info)
        
        logger.info("ü§ó Gerando resposta com Hugging Face...")
        
        # 4. GERAR RESPOSTA COM STREAMING
        full_response = ""
        
        async for chunk in generate_response_stream(prompt, max_length=512):
            full_response += chunk
            yield chunk
        
        # 5. SALVAR EXPERI√äNCIA
        try:
            await save_experience_record(
                user_id=user_id,
                user_message=user_message,
                assistant_response=full_response,
                contexts=contexts,
                prediction_info=prediction_info
            )
            logger.info("üíæ Experi√™ncia salva")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar experi√™ncia: {e}")
    
    except Exception as e:
        logger.error(f"‚ùå Erro na gera√ß√£o de resposta: {e}")
        yield f"Desculpe, ocorreu um erro: {str(e)}"
