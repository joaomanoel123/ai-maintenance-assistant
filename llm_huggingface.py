"""
===============================================================================
HUGGING FACE LLM - Integra√ß√£o com Modelos Hugging Face
===============================================================================

Implementa integra√ß√£o com modelos do Hugging Face:
- Modelos de texto (GPT-2, Llama, Mistral, etc)
- Streaming de tokens
- Infer√™ncia local ou via API

Autor: Jo√£o Manoel
===============================================================================
"""

import os
import logging
from typing import AsyncGenerator, Optional, Dict
import asyncio

logger = logging.getLogger(__name__)

# ============================================
# CONFIGURA√á√ÉO
# ============================================

class HuggingFaceConfig:
    """Configura√ß√£o de modelos Hugging Face"""
    
    # Modelos recomendados (do menor ao maior)
    MODELS = {
        # Modelos pequenos (rodam local em CPU)
        "gpt2": "gpt2",                                    # 124M params
        "gpt2-medium": "gpt2-medium",                      # 355M params
        "distilgpt2": "distilgpt2",                        # 82M params (mais r√°pido)
        
        # Modelos m√©dios (necessitam GPU ou API)
        "flan-t5-base": "google/flan-t5-base",            # 250M params (bom para QA)
        "flan-t5-large": "google/flan-t5-large",          # 780M params
        
        # Modelos grandes (usar via Inference API)
        "mistral-7b": "mistralai/Mistral-7B-Instruct-v0.2",  # 7B params
        "llama-7b": "meta-llama/Llama-2-7b-chat-hf",         # 7B params (requer aprova√ß√£o)
        "zephyr-7b": "HuggingFaceH4/zephyr-7b-beta",         # 7B params (aberto)
    }
    
    # Configura√ß√£o padr√£o
    DEFAULT_MODEL = os.getenv("HF_MODEL", "gpt2")  # Pequeno para come√ßar
    USE_API = os.getenv("HF_USE_API", "false").lower() == "true"
    API_TOKEN = os.getenv("HF_API_TOKEN", None)
    
    # Par√¢metros de gera√ß√£o
    MAX_LENGTH = int(os.getenv("HF_MAX_LENGTH", "200"))
    TEMPERATURE = float(os.getenv("HF_TEMPERATURE", "0.7"))
    TOP_P = float(os.getenv("HF_TOP_P", "0.9"))
    TOP_K = int(os.getenv("HF_TOP_K", "50"))

# ============================================
# MODO 1: INFER√äNCIA LOCAL
# ============================================

class LocalHuggingFaceLLM:
    """
    LLM local usando transformers
    """
    
    def __init__(self, model_name: str = HuggingFaceConfig.DEFAULT_MODEL):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cpu"  # Pode ser "cuda" se tiver GPU
        
    def load(self):
        """Carregar modelo e tokenizer"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"üîÑ Carregando modelo local: {self.model_name}")
            
            # Verificar GPU
            if torch.cuda.is_available():
                self.device = "cuda"
                logger.info("‚úÖ GPU detectada, usando CUDA")
            else:
                logger.info("‚ö†Ô∏è GPU n√£o dispon√≠vel, usando CPU")
            
            # Carregar tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Carregar modelo
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # Configurar pad token se n√£o existir
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"‚úÖ Modelo {self.model_name} carregado no {self.device}")
            return True
            
        except ImportError:
            logger.error("‚ùå transformers ou torch n√£o instalado!")
            logger.error("Instale com: pip install transformers torch")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            return False
    
    async def generate_stream(
        self, 
        prompt: str,
        max_length: int = HuggingFaceConfig.MAX_LENGTH,
        temperature: float = HuggingFaceConfig.TEMPERATURE,
        top_p: float = HuggingFaceConfig.TOP_P,
        top_k: int = HuggingFaceConfig.TOP_K
    ) -> AsyncGenerator[str, None]:
        """
        Gerar resposta com streaming
        """
        if not self.model or not self.tokenizer:
            yield "Erro: Modelo n√£o carregado"
            return
        
        try:
            # Tokenizar input
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # Gerar
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    num_return_sequences=1
                )
            
            # Decodificar
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Remover prompt da resposta
            response = generated_text[len(prompt):].strip()
            
            # Simular streaming (quebrar em palavras)
            words = response.split()
            for word in words:
                yield word + " "
                await asyncio.sleep(0.05)  # Simular lat√™ncia
        
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            yield f"Erro ao gerar resposta: {str(e)}"

# ============================================
# MODO 2: INFERENCE API (RECOMENDADO)
# ============================================

class HuggingFaceInferenceAPI:
    """
    LLM via Hugging Face Inference API (gratuito com rate limit)
    """
    
    def __init__(
        self, 
        model_name: str = HuggingFaceConfig.DEFAULT_MODEL,
        api_token: Optional[str] = None
    ):
        self.model_name = model_name
        self.api_token = api_token or HuggingFaceConfig.API_TOKEN
        self.api_url = f"https://api-inference.huggingface.co/models/{model_name}"
        
        if not self.api_token:
            logger.warning("‚ö†Ô∏è HF_API_TOKEN n√£o configurado, usando modo p√∫blico (rate limit)")
    
    async def generate_stream(
        self,
        prompt: str,
        max_length: int = HuggingFaceConfig.MAX_LENGTH,
        temperature: float = HuggingFaceConfig.TEMPERATURE,
        top_p: float = HuggingFaceConfig.TOP_P
    ) -> AsyncGenerator[str, None]:
        """
        Gerar resposta usando Inference API
        """
        try:
            import httpx
            
            headers = {}
            if self.api_token:
                headers["Authorization"] = f"Bearer {self.api_token}"
            
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_length": max_length,
                    "temperature": temperature,
                    "top_p": top_p,
                    "return_full_text": False
                },
                "options": {
                    "wait_for_model": True
                }
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.api_url,
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Extrair texto gerado
                    if isinstance(result, list) and len(result) > 0:
                        generated_text = result[0].get("generated_text", "")
                    elif isinstance(result, dict):
                        generated_text = result.get("generated_text", "")
                    else:
                        generated_text = str(result)
                    
                    # Simular streaming
                    words = generated_text.split()
                    for word in words:
                        yield word + " "
                        await asyncio.sleep(0.05)
                
                elif response.status_code == 503:
                    yield "Modelo est√° carregando, tente novamente em alguns segundos..."
                    logger.warning("‚ö†Ô∏è Modelo ainda carregando")
                
                else:
                    error_msg = f"Erro API: {response.status_code}"
                    logger.error(f"‚ùå {error_msg}: {response.text}")
                    yield error_msg
        
        except ImportError:
            yield "Erro: httpx n√£o instalado. Execute: pip install httpx"
        except Exception as e:
            logger.error(f"‚ùå Erro na API: {e}")
            yield f"Erro: {str(e)}"

# ============================================
# FACTORY - CRIAR LLM
# ============================================

def create_llm(
    model_name: Optional[str] = None,
    use_api: Optional[bool] = None
) -> Optional[object]:
    """
    Factory para criar inst√¢ncia de LLM
    
    Args:
        model_name: Nome do modelo (padr√£o: config)
        use_api: Usar API ou local (padr√£o: config)
    
    Returns:
        Inst√¢ncia de LLM ou None
    """
    model_name = model_name or HuggingFaceConfig.DEFAULT_MODEL
    use_api = use_api if use_api is not None else HuggingFaceConfig.USE_API
    
    logger.info(f"ü§ó Criando LLM: {model_name} (API: {use_api})")
    
    if use_api:
        # Usar Inference API
        llm = HuggingFaceInferenceAPI(model_name)
        logger.info("‚úÖ LLM criado (Inference API)")
        return llm
    else:
        # Usar modelo local
        llm = LocalHuggingFaceLLM(model_name)
        if llm.load():
            logger.info("‚úÖ LLM criado (Local)")
            return llm
        else:
            logger.error("‚ùå Falha ao carregar modelo local")
            return None

# ============================================
# HELPER - FORMATAR PROMPT
# ============================================

def format_prompt_for_model(
    model_name: str,
    system_prompt: str,
    user_message: str,
    context: Optional[str] = None
) -> str:
    """
    Formatar prompt conforme o modelo
    
    Diferentes modelos t√™m formatos diferentes:
    - GPT-2: texto simples
    - Llama-2: [INST] ... [/INST]
    - Mistral/Zephyr: <|system|> ... <|user|> ...
    """
    
    # GPT-2 e modelos simples
    if "gpt2" in model_name.lower():
        prompt = f"{system_prompt}\n\n"
        if context:
            prompt += f"Context: {context}\n\n"
        prompt += f"User: {user_message}\nAssistant:"
        return prompt
    
    # Llama-2 Chat
    elif "llama" in model_name.lower() and "chat" in model_name.lower():
        prompt = f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n"
        if context:
            prompt += f"{context}\n\n"
        prompt += f"{user_message} [/INST]"
        return prompt
    
    # Mistral/Zephyr
    elif "mistral" in model_name.lower() or "zephyr" in model_name.lower():
        prompt = f"<|system|>\n{system_prompt}</s>\n"
        if context:
            prompt += f"<|assistant|>\n{context}</s>\n"
        prompt += f"<|user|>\n{user_message}</s>\n<|assistant|>"
        return prompt
    
    # FLAN-T5 (diferente - encoder-decoder)
    elif "flan" in model_name.lower():
        prompt = f"{system_prompt}\n\n"
        if context:
            prompt += f"Context: {context}\n\n"
        prompt += f"Question: {user_message}\nAnswer:"
        return prompt
    
    # Fallback gen√©rico
    else:
        prompt = f"{system_prompt}\n\n"
        if context:
            prompt += f"Context: {context}\n\n"
        prompt += f"User: {user_message}\nAssistant:"
        return prompt

# ============================================
# TESTE
# ============================================

async def test_llm():
    """Fun√ß√£o de teste"""
    print("="*80)
    print("üß™ TESTANDO HUGGING FACE LLM")
    print("="*80)
    
    # Criar LLM (API por padr√£o)
    llm = create_llm(model_name="gpt2", use_api=True)
    
    if not llm:
        print("‚ùå Falha ao criar LLM")
        return
    
    # Testar gera√ß√£o
    prompt = "Explain what is machine learning in simple terms:"
    print(f"\nüìù Prompt: {prompt}\n")
    print("ü§ñ Resposta: ", end="", flush=True)
    
    async for token in llm.generate_stream(prompt, max_length=100):
        print(token, end="", flush=True)
    
    print("\n\n‚úÖ Teste conclu√≠do!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_llm())
