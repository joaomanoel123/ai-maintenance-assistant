# main.py - API Principal com Carregamento de Modelos
# Autor: Jo√£o Manoel | Vers√£o: 2.0

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # For√ßar CPU
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from pydantic import BaseModel, EmailStr
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import jwt
from passlib.context import CryptContext
import numpy as np
import pandas as pd
import pickle
import joblib

# Tentar importar TensorFlow (pode n√£o estar instalado)
try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    print("‚ö†Ô∏è  TensorFlow n√£o dispon√≠vel - usando modelos sklearn apenas")

# ==================== CONFIGURA√á√ïES ====================
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-change-in-production")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Caminhos dos modelos
MODEL_PATH_PKL = os.getenv("MODEL_PATH", "./ml-models/cmapss_model.pkl")
MODEL_PATH_H5 = os.getenv("MODEL_PATH_H5", "./ml-models/predictive_maintenance.h5")
SCALER_PATH = os.getenv("SCALER_PATH", "./ml-models/scaler.pkl")

# ==================== FASTAPI APP ====================
app = FastAPI(
    title="Sistema AGI Preditivo",
    version="2.0.0",
    description="Sistema de IA com m√∫ltiplos modelos de Machine Learning"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== VARI√ÅVEIS GLOBAIS DE MODELOS ====================
models_loaded = {
    'sklearn': None,
    'tensorflow': None,
    'scaler': None
}

model_metadata = {
    'sklearn': {'name': 'CMAPSS Model', 'version': 'v1.0', 'loaded': False},
    'tensorflow': {'name': 'CNN-RNN Hybrid', 'version': 'v2.0', 'loaded': False},
    'scaler': {'name': 'StandardScaler', 'loaded': False}
}

# ==================== SCHEMAS ====================
class PredictRequest(BaseModel):
    """Request para predi√ß√£o"""
    temperature: float
    pressure: float
    vibration: float
    rotational_speed: Optional[float] = 1500
    torque: Optional[float] = 40
    tool_wear: Optional[float] = 50

class PredictResponse(BaseModel):
    """Response da predi√ß√£o"""
    prediction: str
    confidence: float
    model_used: str
    timestamp: datetime
    details: Optional[Dict] = None

class ModelInfo(BaseModel):
    """Informa√ß√µes do modelo"""
    name: str
    version: str
    loaded: bool
    type: str

# ==================== CARREGAMENTO DE MODELOS ====================
@app.on_event("startup")
async def load_models():
    """Carrega todos os modelos dispon√≠veis no startup"""
    global models_loaded, model_metadata
    
    print("\n" + "="*70)
    print("üöÄ CARREGANDO MODELOS DE MACHINE LEARNING")
    print("="*70 + "\n")
    
    # 1. Carregar Modelo Sklearn/Joblib
    try:
        if os.path.exists(MODEL_PATH_PKL):
            models_loaded['sklearn'] = joblib.load(MODEL_PATH_PKL)
            model_metadata['sklearn']['loaded'] = True
            print(f"‚úÖ Modelo Sklearn carregado: {MODEL_PATH_PKL}")
        else:
            print(f"‚ö†Ô∏è  Modelo Sklearn n√£o encontrado: {MODEL_PATH_PKL}")
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo Sklearn: {e}")
    
    # 2. Carregar Modelo TensorFlow/Keras
    if TF_AVAILABLE:
        try:
            if os.path.exists(MODEL_PATH_H5):
                models_loaded['tensorflow'] = tf.keras.models.load_model(MODEL_PATH_H5)
                model_metadata['tensorflow']['loaded'] = True
                print(f"‚úÖ Modelo TensorFlow carregado: {MODEL_PATH_H5}")
            else:
                print(f"‚ö†Ô∏è  Modelo TensorFlow n√£o encontrado: {MODEL_PATH_H5}")
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo TensorFlow: {e}")
    else:
        print("‚ÑπÔ∏è  TensorFlow n√£o dispon√≠vel - pulando")
    
    # 3. Carregar Scaler
    try:
        if os.path.exists(SCALER_PATH):
            with open(SCALER_PATH, 'rb') as f:
                models_loaded['scaler'] = pickle.load(f)
            model_metadata['scaler']['loaded'] = True
            print(f"‚úÖ Scaler carregado: {SCALER_PATH}")
        else:
            print(f"‚ö†Ô∏è  Scaler n√£o encontrado: {SCALER_PATH}")
    except Exception as e:
        print(f"‚ùå Erro ao carregar scaler: {e}")
    
    # Resumo
    print("\n" + "="*70)
    print("üìä RESUMO DO CARREGAMENTO")
    print("="*70)
    models_loaded_count = sum(1 for v in models_loaded.values() if v is not None)
    print(f"‚úÖ Total de modelos carregados: {models_loaded_count}/3")
    
    for key, meta in model_metadata.items():
        status_icon = "‚úÖ" if meta['loaded'] else "‚ùå"
        print(f"{status_icon} {meta['name']}: {'Carregado' if meta['loaded'] else 'N√£o carregado'}")
    
    print("="*70 + "\n")
    
    if models_loaded_count == 0:
        print("‚ö†Ô∏è  ATEN√á√ÉO: Nenhum modelo carregado! API funcionar√° em modo limitado.")
    else:
        print("üéâ API pronta para realizar predi√ß√µes!\n")

# ==================== FUN√á√ïES DE PREDI√á√ÉO ====================
def predict_with_sklearn(features: np.ndarray) -> Dict:
    """Predi√ß√£o usando modelo Sklearn"""
    if models_loaded['sklearn'] is None:
        raise HTTPException(status_code=503, detail="Modelo Sklearn n√£o dispon√≠vel")
    
    try:
        prediction = models_loaded['sklearn'].predict(features)[0]
        
        # Tentar obter probabilidades se dispon√≠vel
        confidence = 0.85  # Default
        if hasattr(models_loaded['sklearn'], 'predict_proba'):
            proba = models_loaded['sklearn'].predict_proba(features)[0]
            confidence = float(max(proba))
        
        return {
            'prediction': 'anomaly' if prediction == 1 else 'normal',
            'confidence': confidence,
            'model_used': 'sklearn',
            'raw_prediction': int(prediction)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro na predi√ß√£o Sklearn: {str(e)}")

def predict_with_tensorflow(features: np.ndarray) -> Dict:
    """Predi√ß√£o usando modelo TensorFlow"""
    if models_loaded['tensorflow'] is None:
        raise HTTPException(status_code=503, detail="Modelo TensorFlow n√£o dispon√≠vel")
    
    try:
        # Reshape para CNN-RNN (batch, timesteps, features)
        features_reshaped = features.reshape(1, -1, 1)
        
        prediction_prob = models_loaded['tensorflow'].predict(features_reshaped, verbose=0)[0][0]
        prediction = 'anomaly' if prediction_prob > 0.5 else 'normal'
        confidence = float(prediction_prob if prediction_prob > 0.5 else 1 - prediction_prob)
        
        return {
            'prediction': prediction,
            'confidence': confidence,
            'model_used': 'tensorflow',
            'probability_anomaly': float(prediction_prob),
            'probability_normal': float(1 - prediction_prob)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro na predi√ß√£o TensorFlow: {str(e)}")

def preprocess_input(data: PredictRequest) -> np.ndarray:
    """Pr√©-processa os dados de entrada"""
    features = np.array([[
        data.temperature,
        data.pressure,
        data.vibration,
        data.rotational_speed,
        data.torque,
        data.tool_wear
    ]])
    
    # Aplicar scaler se dispon√≠vel
    if models_loaded['scaler'] is not None:
        features = models_loaded['scaler'].transform(features)
    
    return features

# ==================== ENDPOINTS ====================

@app.get("/")
async def root():
    """Endpoint raiz com status da API"""
    models_status = {
        key: meta['loaded'] 
        for key, meta in model_metadata.items()
    }
    
    return {
        "message": "Sistema AGI Preditivo v2.0",
        "status": "operational",
        "author": "Jo√£o Manoel",
        "models_loaded": models_status,
        "total_models": sum(models_status.values()),
        "endpoints": {
            "predict": "/predict",
            "models": "/models",
            "health": "/health",
            "docs": "/docs"
        }
    }

@app.head("/")
async def head_root():
    """Health check para Render"""
    return {}

@app.get("/health")
async def health_check():
    """Verifica sa√∫de da API e modelos"""
    models_status = []
    
    for key, meta in model_metadata.items():
        models_status.append({
            "name": meta['name'],
            "version": meta.get('version', 'N/A'),
            "loaded": meta['loaded'],
            "type": key
        })
    
    all_loaded = all(meta['loaded'] for meta in model_metadata.values())
    
    return {
        "status": "healthy" if any(meta['loaded'] for meta in model_metadata.values()) else "degraded",
        "timestamp": datetime.utcnow(),
        "models": models_status,
        "all_models_loaded": all_loaded
    }

@app.get("/models", response_model=List[ModelInfo])
async def get_models_info():
    """Lista todos os modelos dispon√≠veis"""
    models = []
    
    for key, meta in model_metadata.items():
        models.append(ModelInfo(
            name=meta['name'],
            version=meta.get('version', 'N/A'),
            loaded=meta['loaded'],
            type=key
        ))
    
    return models

@app.post("/predict", response_model=PredictResponse)
async def predict(data: PredictRequest):
    """
    Realiza predi√ß√£o usando o melhor modelo dispon√≠vel
    
    Priority:
    1. TensorFlow (mais preciso)
    2. Sklearn (mais r√°pido)
    """
    
    # Verificar se h√° algum modelo carregado
    if not any(models_loaded.values()):
        raise HTTPException(
            status_code=503,
            detail="Nenhum modelo dispon√≠vel. Verifique os logs de startup."
        )
    
    try:
        # Pr√©-processar entrada
        features = preprocess_input(data)
        
        # Tentar usar TensorFlow primeiro (mais preciso)
        if models_loaded['tensorflow'] is not None:
            result = predict_with_tensorflow(features)
        
        # Fallback para Sklearn
        elif models_loaded['sklearn'] is not None:
            result = predict_with_sklearn(features)
        
        else:
            raise HTTPException(
                status_code=503,
                detail="Nenhum modelo de predi√ß√£o dispon√≠vel"
            )
        
        # Adicionar recomenda√ß√£o
        recommendation = "Continue opera√ß√£o normal"
        if result['prediction'] == 'anomaly':
            if result['confidence'] > 0.9:
                recommendation = "‚ö†Ô∏è CR√çTICO: Parar equipamento e inspecionar imediatamente"
            elif result['confidence'] > 0.7:
                recommendation = "‚ö†Ô∏è ALERTA: Agendar manuten√ß√£o preventiva urgente"
            else:
                recommendation = "‚ö†Ô∏è Monitorar de perto e verificar em 24h"
        
        return PredictResponse(
            prediction=result['prediction'],
            confidence=result['confidence'],
            model_used=result['model_used'],
            timestamp=datetime.utcnow(),
            details={
                **result,
                'recommendation': recommendation,
                'input_data': {
                    'temperature': data.temperature,
                    'pressure': data.pressure,
                    'vibration': data.vibration,
                    'rpm': data.rotational_speed,
                    'torque': data.torque,
                    'tool_wear': data.tool_wear
                }
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erro durante predi√ß√£o: {str(e)}"
        )

@app.post("/predict/batch")
async def predict_batch(data: List[PredictRequest]):
    """Predi√ß√£o em lote para m√∫ltiplas amostras"""
    if len(data) > 100:
        raise HTTPException(
            status_code=400,
            detail="M√°ximo de 100 amostras por requisi√ß√£o"
        )
    
    results = []
    for item in data:
        try:
            result = await predict(item)
            results.append(result)
        except Exception as e:
            results.append({
                "error": str(e),
                "input": item.dict()
            })
    
    return {
        "total": len(data),
        "success": len([r for r in results if not isinstance(r, dict) or 'error' not in r]),
        "failed": len([r for r in results if isinstance(r, dict) and 'error' in r]),
        "results": results
    }

@app.get("/models/reload")
async def reload_models():
    """Recarrega todos os modelos (√∫til ap√≥s upload)"""
    await load_models()
    return {
        "message": "Modelos recarregados",
        "status": [
            {key: meta['loaded']} 
            for key, meta in model_metadata.items()
        ]
    }

# ==================== EXECUTAR ====================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
