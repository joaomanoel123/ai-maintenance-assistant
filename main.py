"""
===============================================================================
MAIN CHAT API - Pipeline AGI v2.0 com Mem√≥ria Vetorial
===============================================================================

Backend completo com:
- WebSocket para chat streaming
- Mem√≥ria vetorial (ChromaDB)
- Modelos preditivos integrados
- Ingest√£o de datasets Kaggle
- Persist√™ncia no Neon PostgreSQL

Autor: Jo√£o Manoel
Deploy: Render.com
===============================================================================
"""

import os
import asyncio
import logging
from datetime import datetime
from typing import Optional, Dict, Any

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Imports dos m√≥dulos customizados
from model_loader import load_models, MODELS, model_predict
from embeddings import VectorMemory
from chat_pipeline import respond_stream_generator, extract_prediction_intent
from db import save_experience_record, init_database

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# INICIALIZAR APP
# ============================================

app = FastAPI(
    title="AGI Chat + Predictive API",
    version="2.1",
    description="Sistema AGI com chat responsivo e mem√≥ria vetorial"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mem√≥ria vetorial global
memory: Optional[VectorMemory] = None

# ============================================
# MODELS
# ============================================

class PredictRequest(BaseModel):
    type: str  # 'cmapss' ou 'ai4i'
    features: list

class ChatMessage(BaseModel):
    message: str
    user_id: str = "anonymous"

# ============================================
# STARTUP
# ============================================

@app.on_event("startup")
async def startup_event():
    """Inicializar modelos e mem√≥ria vetorial"""
    logger.info("üöÄ Iniciando AGI Chat API...")
    
    try:
        # 1. Carregar modelos preditivos
        logger.info("üì¶ Carregando modelos preditivos...")
        load_models()
        logger.info(f"‚úÖ Modelos carregados: {list(MODELS.keys())}")
        
        # 2. Inicializar banco de dados
        logger.info("üóÑÔ∏è Inicializando banco de dados...")
        await init_database()
        logger.info("‚úÖ Database pronto")
        
        # 3. Inicializar mem√≥ria vetorial
        logger.info("üß† Inicializando mem√≥ria vetorial...")
        global memory
        memory = VectorMemory(collection_name="agi_memory")
        await memory.start()
        logger.info("‚úÖ Mem√≥ria vetorial inicializada")
        
        # 4. Verificar se precisa ingest√£o inicial
        doc_count = memory.get_collection_size()
        if doc_count == 0:
            logger.info("üìö Mem√≥ria vazia, executando ingest√£o inicial...")
            await run_initial_ingestion()
        else:
            logger.info(f"‚úÖ Mem√≥ria j√° possui {doc_count} documentos")
        
        logger.info("üéâ Startup conclu√≠do com sucesso!")
        
    except Exception as e:
        logger.error(f"‚ùå Erro no startup: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup"""
    logger.info("üëã Encerrando AGI Chat API...")
    if memory:
        await memory.close()

# ============================================
# ENDPOINTS REST
# ============================================

@app.get("/")
def read_root():
    """Endpoint raiz"""
    return {
        "status": "ok",
        "message": "AGI Chat + Predictive API",
        "version": "2.1",
        "endpoints": {
            "predict": "/predict",
            "chat_ws": "/ws-chat",
            "health": "/health",
            "memory_stats": "/memory/stats"
        }
    }

@app.get("/health")
async def health_check():
    """Health check"""
    models_status = {k: v is not None for k, v in MODELS.items()}
    memory_status = memory is not None and memory.collection is not None
    
    return {
        "status": "healthy",
        "models": models_status,
        "memory": memory_status,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict")
async def predict(payload: PredictRequest):
    """
    Endpoint de predi√ß√£o (REST)
    """
    model_key = payload.type
    
    if model_key not in MODELS:
        raise HTTPException(
            status_code=400, 
            detail=f"Modelo '{model_key}' n√£o dispon√≠vel. Op√ß√µes: {list(MODELS.keys())}"
        )
    
    model = MODELS[model_key]
    if model is None:
        raise HTTPException(
            status_code=500, 
            detail=f"Modelo '{model_key}' n√£o carregado"
        )
    
    try:
        prediction = model_predict(model_key, payload.features)
        
        return {
            "model": model_key,
            "prediction": prediction,
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        logger.error(f"Erro na predi√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/memory/stats")
async def get_memory_stats():
    """Estat√≠sticas da mem√≥ria vetorial"""
    if not memory:
        raise HTTPException(status_code=503, detail="Mem√≥ria n√£o inicializada")
    
    return {
        "collection_name": memory.collection_name,
        "total_documents": memory.get_collection_size(),
        "embedding_model": "all-MiniLM-L6-v2",
        "status": "active"
    }

@app.post("/memory/add")
async def add_to_memory(data: Dict[str, Any]):
    """Adicionar documento √† mem√≥ria"""
    if not memory:
        raise HTTPException(status_code=503, detail="Mem√≥ria n√£o inicializada")
    
    try:
        doc_id = data.get("id", f"doc_{datetime.now().timestamp()}")
        document = data.get("document")
        metadata = data.get("metadata", {})
        
        if not document:
            raise HTTPException(status_code=400, detail="Campo 'document' obrigat√≥rio")
        
        memory.add_documents(
            ids=[doc_id],
            documents=[document],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "id": doc_id,
            "message": "Documento adicionado √† mem√≥ria"
        }
    
    except Exception as e:
        logger.error(f"Erro ao adicionar √† mem√≥ria: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# WEBSOCKET CHAT
# ============================================

@app.websocket("/ws-chat")
async def websocket_chat(websocket: WebSocket):
    """
    WebSocket para chat com streaming
    """
    await websocket.accept()
    logger.info("‚úÖ Cliente conectado ao chat")
    
    try:
        while True:
            # Receber mensagem
            data = await websocket.receive_json()
            user_message = data.get("message", "")
            user_id = data.get("user_id", "anonymous")
            
            if not user_message:
                await websocket.send_json({
                    "type": "error",
                    "data": "Mensagem vazia"
                })
                continue
            
            logger.info(f"üì• Mensagem recebida de {user_id}: {user_message[:50]}...")
            
            # Verificar mem√≥ria
            if not memory:
                await websocket.send_json({
                    "type": "error",
                    "data": "Mem√≥ria n√£o dispon√≠vel"
                })
                continue
            
            # Gerar resposta com streaming
            try:
                async for chunk in respond_stream_generator(
                    user_message, 
                    user_id, 
                    memory,
                    MODELS
                ):
                    await websocket.send_json({
                        "type": "token",
                        "data": chunk
                    })
                
                # Sinal de fim
                await websocket.send_json({
                    "type": "end",
                    "data": "done"
                })
                
                logger.info(f"‚úÖ Resposta enviada para {user_id}")
            
            except Exception as e:
                logger.error(f"Erro ao gerar resposta: {e}")
                await websocket.send_json({
                    "type": "error",
                    "data": f"Erro ao processar: {str(e)}"
                })
    
    except WebSocketDisconnect:
        logger.info("üëã Cliente desconectado")
    
    except Exception as e:
        logger.error(f"‚ùå Erro no WebSocket: {e}")

# ============================================
# INGEST√ÉO INICIAL
# ============================================

async def run_initial_ingestion():
    """
    Ingest√£o inicial de conhecimento base
    """
    logger.info("üìö Executando ingest√£o inicial...")
    
    # Conhecimento base sobre o sistema
    base_knowledge = [
        {
            "id": "system_intro",
            "document": "Sistema AGI Generativa v2.0 com capacidades de predi√ß√£o (CMAPSS e AI4I), racioc√≠nio cognitivo, tomada de decis√£o e aprendizado por feedback (RLHF).",
            "metadata": {"category": "system", "priority": "high"}
        },
        {
            "id": "cmapss_info",
            "document": "CMAPSS √© o modelo de predi√ß√£o de RUL (Remaining Useful Life) que estima a vida √∫til restante de motores turbofan usando dados de 21 sensores ao longo do tempo.",
            "metadata": {"category": "models", "type": "cmapss"}
        },
        {
            "id": "ai4i_info",
            "document": "AI4I √© o modelo de predi√ß√£o de falhas em m√°quinas industriais que analisa temperatura, rota√ß√£o, torque e desgaste para prever probabilidade de falha.",
            "metadata": {"category": "models", "type": "ai4i"}
        },
        {
            "id": "rul_definition",
            "document": "RUL (Remaining Useful Life) √© a estimativa de quanto tempo ou ciclos operacionais restam antes de uma manuten√ß√£o ser necess√°ria.",
            "metadata": {"category": "concepts", "term": "rul"}
        },
        {
            "id": "prediction_process",
            "document": "O processo de predi√ß√£o envolve: 1) Coleta de dados dos sensores, 2) Normaliza√ß√£o, 3) Predi√ß√£o usando modelo CNN-RNN ou Random Forest, 4) An√°lise de racioc√≠nio, 5) Recomenda√ß√£o de a√ß√£o.",
            "metadata": {"category": "process"}
        },
        {
            "id": "rlhf_info",
            "document": "RLHF (Reinforcement Learning from Human Feedback) √© o mecanismo de aprendizado cont√≠nuo onde o sistema melhora suas respostas baseado no feedback dos usu√°rios.",
            "metadata": {"category": "features", "type": "rlhf"}
        },
        {
            "id": "modules_info",
            "document": "Os m√≥dulos AGI incluem: Mem√≥ria (curto e longo prazo), Racioc√≠nio (causal, temporal, indutivo), Decis√£o (orientada a metas), Gera√ß√£o (explica√ß√µes textuais) e Metacogni√ß√£o.",
            "metadata": {"category": "architecture"}
        }
    ]
    
    ids = [item["id"] for item in base_knowledge]
    documents = [item["document"] for item in base_knowledge]
    metadatas = [item["metadata"] for item in base_knowledge]
    
    memory.add_documents(ids=ids, documents=documents, metadatas=metadatas)
    
    logger.info(f"‚úÖ Ingest√£o conclu√≠da: {len(base_knowledge)} documentos adicionados")

# ============================================
# INGEST√ÉO DE DATASETS KAGGLE (OPCIONAL)
# ============================================

@app.post("/ingest/kaggle")
async def ingest_kaggle_datasets():
    """
    Endpoint para ingerir datasets do Kaggle
    (Executar manualmente ou via cron job)
    """
    if not memory:
        raise HTTPException(status_code=503, detail="Mem√≥ria n√£o inicializada")
    
    try:
        from ingest_datasets import ingest_ai4i_sample, ingest_cmapss_sample
        
        logger.info("üìä Iniciando ingest√£o de datasets Kaggle...")
        
        # Ingerir AI4I
        await ingest_ai4i_sample(memory)
        
        # Ingerir CMAPSS
        await ingest_cmapss_sample(memory)
        
        total_docs = memory.get_collection_size()
        
        return {
            "status": "success",
            "message": "Datasets ingeridos com sucesso",
            "total_documents": total_docs
        }
    
    except Exception as e:
        logger.error(f"Erro na ingest√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# EXECU√á√ÉO
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    
    logger.info("="*80)
    logger.info("üöÄ AGI CHAT API v2.1")
    logger.info("="*80)
    logger.info(f"üì° Porta: {port}")
    logger.info(f"üîó WebSocket: ws://localhost:{port}/ws-chat")
    logger.info(f"üìö Docs: http://localhost:{port}/docs")
    logger.info("="*80)
    
    uvicorn.run(
        "main_chat:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )
